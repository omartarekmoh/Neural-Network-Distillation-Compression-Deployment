{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":252310,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":215712,"modelId":237420}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install optuna onnx onnx_tf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Il5cZKtMzsDi","outputId":"22961ebb-fe67-4f6a-ae25-945b87ff783b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models, datasets\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Subset\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport os\nimport optuna\nfrom tqdm import tqdm\nimport torch.nn.functional as F","metadata":{"id":"E4cNJzWO5zgG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ImageNet Normalization values\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Define transformations\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images for ResNet50\n    transforms.RandomHorizontalFlip(),  # Data Augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\n# Define number of samples\nnum_train_samples = 1500\nnum_test_samples = 500\n\n# Get indices for the subset\ntrain_indices = np.random.choice(len(trainset), num_train_samples, replace=False)\ntest_indices = np.random.choice(len(testset), num_test_samples, replace=False)\n\n# Create subset datasets\ntrain_subset = Subset(trainset, train_indices[:1000])\nval_subset = Subset(trainset, train_indices[1000:])\ntest_subset = Subset(testset, test_indices)\n\n# Create DataLoaders\nbatch_size = 128\n\n# Load training data\ntrainloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n# Load training data\nvalloader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n\n# Load test data\ntestloader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I7cb_7QM6Mam","outputId":"5ac17db1-80d4-4ccb-80f1-616dd0bbe436","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(trainloader)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fndH3xEKEDv5","outputId":"5acfd0fc-eb07-4b04-93cd-536910c536cc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get class names\nclasses = trainset.classes\nprint(classes)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPXYXSSWAPi4","outputId":"a3806515-cc7a-4bba-8abf-5b6ebb68de46","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainset[0][0].shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmXvbmgC99NX","outputId":"42715586-a265-4f73-c227-ad2233302e32","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define mean and std used for normalization (e.g., CIFAR-10 normalization)\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ndef denormalize(img):\n    \"\"\"Undo normalization: img * std + mean\"\"\"\n    img = img * std[:] + mean[:]  # Unnormalize\n    return np.clip(img, 0, 1)  # Clip values to [0,1] for imshow()\n\ndef show_images_in_rows(images, labels, num_rows=1, num_cols=5):\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n    axes = axes.flatten()\n\n    for i in range(num_rows * num_cols):\n        if i < len(images):  # Ensure we don't go out of bounds\n            img = images[i].numpy().transpose((1, 2, 0))  # Convert (C, H, W) -> (H, W, C)\n            img = denormalize(img)  # Denormalize\n            axes[i].imshow(img)\n            axes[i].set_title(f\"Label: {labels[i]}\")\n            axes[i].axis('off')  # Hide axis\n        else:\n            axes[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Get a small batch of 4 images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Select only the first 4 images\nimages, labels = images[:4], labels[:4]\n\n# Show images\nshow_images_in_rows(images, [classes[labels[j].item()] for j in range(4)], 1, 4)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"Q7KTNetZ8ffy","outputId":"2cbc6336-97e2-45b6-f0d7-afc53548b394","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomResNet(nn.Module):\n    def __init__(self, num_classes, intermediate_dim, dropout1_rate, dropout2_rate):\n        super(CustomResNet, self).__init__()\n        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        num_ftrs = self.resnet.fc.in_features\n\n        self.resnet.fc = nn.Sequential(\n            nn.Linear(num_ftrs, intermediate_dim),\n            nn.Dropout(p=dropout1_rate),\n            nn.ReLU(),\n            nn.Dropout(p=dropout2_rate),\n            nn.Linear(intermediate_dim, num_classes)\n        )\n\n    def forward(self, x):\n        return self.resnet(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(trial, num_classes):\n    # Suggest hyperparameters for the model architecture\n    intermediate_dim = trial.suggest_int('intermediate_dim', 256, 2048, step=256)\n    dropout1_rate = trial.suggest_float('dropout1_rate', 0.1, 0.5)\n    dropout2_rate = trial.suggest_float('dropout2_rate', 0.1, 0.5)\n\n    model = CustomResNet(\n        num_classes=num_classes,\n        intermediate_dim=intermediate_dim,\n        dropout1_rate=dropout1_rate,\n        dropout2_rate=dropout2_rate\n    )\n    return model\n\ndef objective(trial, train_dataset, val_dataset, num_classes):\n    # Define hyperparameters to optimize\n    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2\n    )\n\n    # Initialize model with architecture hyperparameters\n    model = create_model(trial, num_classes)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    # Set up optimizer based on trial suggestion\n    if optimizer_name == 'Adam':\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n    else:\n        momentum = trial.suggest_float('momentum', 0.1, 0.9)\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop\n    num_epochs = trial.suggest_int('num_epochs', 5, 15)\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        for images, labels in trainloader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Validation phase\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for images, labels in valloader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        accuracy = correct / total\n\n        # Report intermediate values to Optuna\n        trial.report(accuracy, epoch)\n\n        # Handle pruning based on the intermediate value\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    return accuracy\n\ndef run_optuna_optimization(train_dataset, val_dataset, num_classes, n_trials=100):\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5),\n        study_name=\"resnet50_optimization\"\n    )\n\n    study.optimize(\n        lambda trial: objective(trial, train_dataset, val_dataset, num_classes),\n        n_trials=n_trials\n    )\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    return study.best_params","metadata":{"id":"_3Wtod-LycR2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = run_optuna_optimization(\n    train_dataset=train_subset,\n    val_dataset=val_subset,\n    num_classes=len(classes),\n    n_trials=20\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcHF28MnAmmX","outputId":"3b82d25d-6412-4850-9a2e-4c3a0af03361","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_final_model(train_dataset, val_dataset, best_params, num_classes, save_path='best_model.pth'):\n    # Create model with best parameters\n    model = CustomResNet(\n        num_classes=num_classes,\n        intermediate_dim=best_params['intermediate_dim'],\n        dropout1_rate=best_params['dropout1_rate'],\n        dropout2_rate=best_params['dropout2_rate']\n    )\n\n    # Create data loaders with best batch size\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=best_params['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=best_params['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n\n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    # Setup optimizer with best parameters\n    if best_params['optimizer'] == 'Adam':\n        optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n    else:\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=best_params['lr'],\n            momentum=best_params['momentum']\n        )\n\n    criterion = nn.CrossEntropyLoss()\n    num_epochs = best_params['num_epochs']\n\n    # Keep track of best validation accuracy\n    best_val_acc = 0.0\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n\n            # Update progress bar\n            progress_bar.set_postfix({\n                'loss': f'{train_loss/train_total:.4f}',\n                'acc': f'{100.*train_correct/train_total:.2f}%'\n            })\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        # Calculate metrics\n        train_accuracy = 100. * train_correct / train_total\n        val_accuracy = 100. * val_correct / val_total\n\n        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n        print(f'Training Loss: {train_loss/len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%')\n        print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n\n        # Save best model\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            print(f'New best validation accuracy: {best_val_acc:.2f}%')\n            print(f'Saving model to {save_path}')\n\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n\n            # Save the model\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_val_acc': best_val_acc,\n                'hyperparameters': best_params\n            }, save_path)\n\n    print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n    return model\n\n# Then train the final model with best parameters\nfinal_model = train_final_model(\n    train_dataset=train_subset,\n    val_dataset=val_subset,\n    best_params=best_params,\n    num_classes=len(classes),\n    save_path='models/best_resnet50_model.pth'\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSwGVXObFAdA","outputId":"a4bf186c-dfbe-4167-d4b6-4d4635ae4cec","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize lists to store true labels and predictions\nall_preds = []\nall_labels = []\n\n#  num_classes, intermediate_dim, dropout1_rate, dropout2_rate\n# Best trial:\n#   Value:  0.86\n#   Params:\n#     batch_size: 80\n#     lr: 9.035438144665984e-05\n#     optimizer: Adam\n#     intermediate_dim: 1024\n#     dropout1_rate: 0.3055373006787046\n#     dropout2_rate: 0.10413266283398273\n#     num_epochs: 15\ncheckpoint = torch.load(\"/kaggle/input/best_resnet50_for_cifar10/pytorch/default/1/best_resnet50_model.pth\")\n\nmodel = CustomResNet(10,\n                     checkpoint[\"hyperparameters\"][\"intermediate_dim\"],\n                     checkpoint[\"hyperparameters\"][\"dropout1_rate\"],\n                     checkpoint[\"hyperparameters\"][\"dropout2_rate\"]\n                     )\noptimizer = torch.optim.Adam(model.parameters(), checkpoint[\"hyperparameters\"]['lr'])\n\n# Load saved state dictionaries\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# Restore additional information\nepoch = checkpoint['epoch']\nbest_val_acc = checkpoint['best_val_acc']\nhyperparameters = checkpoint['hyperparameters']\n\nprint(f\"Model loaded from epoch {epoch} with best validation accuracy: {best_val_acc}\")","metadata":{"id":"bCW6HFXVFBLf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d7cf0aa-f7e8-43b7-9c60-4544d499f4bc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\nmodel.to(device)\nmodel.eval()  # Set model to evaluation mode\n\nwith torch.no_grad():  # No need to calculate gradients\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)  # Forward pass\n        _, predicted = torch.max(outputs, 1)  # Get the index of the highest score\n\n        all_preds.extend(predicted.cpu().numpy())  # Store predictions\n        all_labels.extend(labels.cpu().numpy())  # Store true labels","metadata":{"id":"Rv9LqUvJDYYp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay\n\n# Create confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Plot confusion matrix\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":629},"id":"BW4ghsB3Fa5o","outputId":"ce3f26dc-bcfa-487a-db12-ba4dc61e45c5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to store misclassified samples\nwrong_images = []\nwrong_preds = []\nwrong_labels = []\n\n# Evaluate the model\nmodel.eval()  # Set model to evaluation mode\n\nwith torch.no_grad():  # No need to calculate gradients\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)  # Forward pass\n        _, predicted = torch.max(outputs, 1)  # Get predicted label\n\n        # Find misclassified samples\n        mask = predicted != labels\n        if mask.any():  # If there are any wrong predictions\n            wrong_images.extend(images[mask].cpu())  # Store wrong images\n            wrong_preds.extend(predicted[mask].cpu().numpy())  # Store wrong predictions\n            wrong_labels.extend(labels[mask].cpu().numpy())  # Store actual labels\n\n\ndef show_misclassified_images(images, true_labels, pred_labels, num_images=10):\n    num_images = min(num_images, len(images))  # Ensure we don't go out of bounds\n    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 2))\n\n    for i in range(num_images):\n        img = images[i].numpy().transpose((1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n        img = img * std + mean  # Denormalize\n        img = np.clip(img, 0, 1)  # Clip values for display\n\n        axes[i].imshow(img)\n        axes[i].set_title(f\"T: {classes[true_labels[i]]}\\nP: {classes[pred_labels[i]]}\", fontsize=10)\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Show misclassified images\nshow_misclassified_images(wrong_images, wrong_labels, wrong_preds, num_images=10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153},"id":"Nzri8moOFcoy","outputId":"f4c94347-5fbb-4c5e-8272-b388930d220b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StudentResNet(nn.Module):\n    def __init__(self, num_classes, intermediate_dim, dropout1_rate, dropout2_rate):\n        super(StudentResNet, self).__init__()\n        # Using ResNet18 as student (smaller than ResNet50)\n        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        num_ftrs = self.resnet.fc.in_features\n\n        # Same architecture as teacher for the final layers\n        self.resnet.fc = nn.Sequential(\n            nn.Linear(num_ftrs, intermediate_dim),\n            nn.Dropout(p=dropout1_rate),\n            nn.ReLU(),\n            nn.Dropout(p=dropout2_rate),\n            nn.Linear(intermediate_dim, num_classes)\n        )\n\n    def forward(self, x):\n        return self.resnet(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def kd_loss(student_logits, teacher_logits, labels, temperature=3, alpha=0.7):\n    \"\"\"\n    Knowledge Distillation Loss: combines KL Divergence and CrossEntropy Loss.\n    \"\"\"\n    soft_targets = F.log_softmax(student_logits / temperature, dim=1)\n    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)\n\n    kd_loss = F.kl_div(soft_targets, soft_teacher, reduction='batchmean') * (temperature ** 2)\n    ce_loss = nn.CrossEntropyLoss()(student_logits, labels)\n\n    return alpha * kd_loss + (1 - alpha) * ce_loss","metadata":{"id":"dANkVk8NFQwh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial, teacher_model, train_dataset, val_dataset, num_classes):\n    # Architecture hyperparameters\n    intermediate_dim = trial.suggest_int('intermediate_dim', 128, 1024, step=128)\n    dropout1_rate = trial.suggest_float('dropout1_rate', 0.1, 0.5)\n    dropout2_rate = trial.suggest_float('dropout2_rate', 0.1, 0.5)\n\n    # Distillation hyperparameters\n    temperature = trial.suggest_float('temperature', 1.0, 10.0)\n    alpha = trial.suggest_float('alpha', 0.1, 0.9)\n\n    # Training hyperparameters\n    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n\n    # Create student model\n    student_model = StudentResNet(\n        num_classes=num_classes,\n        intermediate_dim=intermediate_dim,\n        dropout1_rate=dropout1_rate,\n        dropout2_rate=dropout2_rate\n    )\n\n    # Setup device and move models\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    student_model = student_model.to(device)\n    teacher_model = teacher_model.to(device)\n    teacher_model.eval()\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    # Setup optimizer\n    if optimizer_name == 'Adam':\n        optimizer = optim.Adam(student_model.parameters(), lr=lr)\n    else:\n        momentum = trial.suggest_float('momentum', 0.1, 0.9)\n        optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n\n    # Training loop\n    num_epochs = trial.suggest_int('num_epochs', 5, 15)\n\n    for epoch in range(num_epochs):\n        student_model.train()\n        running_loss = 0.0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Get teacher predictions\n            with torch.no_grad():\n                teacher_logits = teacher_model(images)\n\n            # Forward pass student\n            student_logits = student_model(images)\n\n            # Calculate loss\n            loss = kd_loss(\n                student_logits=student_logits,\n                teacher_logits=teacher_logits,\n                labels=labels,\n                temperature=temperature,\n                alpha=alpha\n            )\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        # Validation phase\n        student_model.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = student_model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        accuracy = correct / total\n        trial.report(accuracy, epoch)\n\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    return accuracy","metadata":{"id":"npnsk3YuFtgE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_distillation_optimization(teacher_model, train_dataset, val_dataset, num_classes, n_trials=100):\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5),\n        study_name=\"distillation_optimization\"\n    )\n\n    study.optimize(\n        lambda trial: objective(trial, teacher_model, train_dataset, val_dataset, num_classes),\n        n_trials=n_trials\n    )\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: \", trial.value)\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n\n    return study.best_params","metadata":{"id":"QLAlOP-3FuRt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = run_distillation_optimization(\n    teacher_model=model,\n    train_dataset=train_subset,\n    val_dataset=val_subset,\n    num_classes=len(classes),\n    n_trials=20\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olMqYFH-F-RF","outputId":"150292d6-f97b-41fc-87ce-27b697091f09","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_final_student(teacher_model, train_dataset, val_dataset, best_params, num_classes, save_path='best_student.pth'):\n    # Create student model with best parameters\n    student_model = StudentResNet(\n        num_classes=num_classes,\n        intermediate_dim=best_params['intermediate_dim'],\n        dropout1_rate=best_params['dropout1_rate'],\n        dropout2_rate=best_params['dropout2_rate']\n    )\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    student_model = student_model.to(device)\n    teacher_model = teacher_model.to(device)\n    teacher_model.eval()\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=best_params['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=best_params['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n\n    # Setup optimizer\n    if best_params['optimizer'] == 'Adam':\n        optimizer = optim.Adam(student_model.parameters(), lr=best_params['lr'])\n    else:\n        optimizer = optim.SGD(\n            student_model.parameters(),\n            lr=best_params['lr'],\n            momentum=best_params['momentum']\n        )\n\n    best_val_acc = 0.0\n\n    for epoch in range(best_params['num_epochs']):\n        # Training phase\n        student_model.train()\n        train_loss = 0.0\n\n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{best_params[\"num_epochs\"]}')\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device)\n\n            # Get teacher predictions\n            with torch.no_grad():\n                teacher_logits = teacher_model(images)\n\n            # Forward pass student\n            student_logits = student_model(images)\n\n            # Calculate loss\n            loss = kd_loss(\n                student_logits=student_logits,\n                teacher_logits=teacher_logits,\n                labels=labels,\n                temperature=best_params['temperature'],\n                alpha=best_params['alpha']\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        # Validation phase\n        student_model.eval()\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = student_model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n\n        val_accuracy = 100. * val_correct / val_total\n        print(f'\\nEpoch {epoch+1}: Validation Accuracy: {val_accuracy:.2f}%')\n\n        # Save best model\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            print(f'New best validation accuracy: {best_val_acc:.2f}%')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': student_model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_val_acc': best_val_acc,\n                'hyperparameters': best_params\n            }, save_path)\n\n    print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n    return student_model","metadata":{"id":"KUHc0UZ3H11_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndirectory = \"models\"\nos.makedirs(directory, exist_ok=True) \nprint(f\"Directory '{directory}' created successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_student = train_final_student(\n    teacher_model=model,\n    train_dataset=train_subset,\n    val_dataset=val_subset,\n    best_params=best_params,\n    num_classes=len(classes),\n    save_path='/kaggle/working/models/best_student.pth'\n)","metadata":{"id":"JFFSp2qIG5FV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize lists to store true labels and predictions\nall_preds = []\nall_labels = []\n\n#  num_classes, intermediate_dim, dropout1_rate, dropout2_rate\n\n# Best trial:\n#   Value:  0.86\n#   Params:\n#     batch_size: 80\n#     lr: 9.035438144665984e-05\n#     optimizer: Adam\n#     intermediate_dim: 1024\n#     dropout1_rate: 0.3055373006787046\n#     dropout2_rate: 0.10413266283398273\n#     num_epochs: 15\ncheckpoint = torch.load(\"/kaggle/working/models/best_student.pth\")\n\nstudent = StudentResNet(10,\n                     checkpoint[\"hyperparameters\"][\"intermediate_dim\"],\n                     checkpoint[\"hyperparameters\"][\"dropout1_rate\"],\n                     checkpoint[\"hyperparameters\"][\"dropout2_rate\"]\n                     )\noptimizer = torch.optim.SGD(student.parameters(), checkpoint[\"hyperparameters\"]['lr'])\n\n# Load saved state dictionaries\nstudent.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# Restore additional information\nepoch = checkpoint['epoch']\nbest_val_acc = checkpoint['best_val_acc']\nhyperparameters = checkpoint['hyperparameters']\n\nprint(f\"Model loaded from epoch {epoch} with best validation accuracy: {best_val_acc}\")","metadata":{"id":"_-v3O8zUHCB_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to store true labels and predictions\nall_preds = []\nall_labels = []\n\n# Evaluate the student\nstudent.to(device)\nstudent.eval()  # Set student to evaluation mode\n\nwith torch.no_grad():  # No need to calculate gradients\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = student(images)  # Forward pass\n        _, predicted = torch.max(outputs, 1)  # Get the index of the highest score\n\n        all_preds.extend(predicted.cpu().numpy())  # Store predictions\n        all_labels.extend(labels.cpu().numpy())  # Store true labels","metadata":{"id":"msUnHcSdK2wq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Plot confusion matrix\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"id":"1ctxiTa_LWeL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to store misclassified samples\nwrong_images = []\nwrong_preds = []\nwrong_labels = []\n\n# Evaluate the student\nstudent.eval()  # Set student to evaluation mode\n\nwith torch.no_grad():  # No need to calculate gradients\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = student(images)  # Forward pass\n        _, predicted = torch.max(outputs, 1)  # Get predicted label\n\n        # Find misclassified samples\n        mask = predicted != labels\n        if mask.any():  # If there are any wrong predictions\n            wrong_images.extend(images[mask].cpu())  # Store wrong images\n            wrong_preds.extend(predicted[mask].cpu().numpy())  # Store wrong predictions\n            wrong_labels.extend(labels[mask].cpu().numpy())  # Store actual labels\n\n\ndef show_misclassified_images(images, true_labels, pred_labels, num_images=10):\n    num_images = min(num_images, len(images))  # Ensure we don't go out of bounds\n    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 2))\n\n    for i in range(num_images):\n        img = images[i].numpy().transpose((1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n        img = img * std + mean  # Denormalize\n        img = np.clip(img, 0, 1)  # Clip values for display\n\n        axes[i].imshow(img)\n        axes[i].set_title(f\"T: {classes[true_labels[i]]}\\nP: {classes[pred_labels[i]]}\", fontsize=10)\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Show misclassified images\nshow_misclassified_images(wrong_images, wrong_labels, wrong_preds, num_images=10)","metadata":{"id":"3FtxUU2-LdDc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import onnx\n\n# Ensure the model is on the same device as the input\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nstudent = student.to(device)  # Move model to GPU/CPU\n\n# Create dummy input and move it to the same device as the model\ndummy_input = torch.randn(1, 3, 224, 224).to(device)  # Match model's input shape\n\n# Define the ONNX save path\nonnx_path = \"/kaggle/working/models/onnx_model.onnx\"\n\n# Export the model to ONNX\ntorch.onnx.export(student, dummy_input, onnx_path,\n                  export_params=True,\n                  opset_version=11,\n                  do_constant_folding=True,\n                  input_names=['input'],\n                  output_names=['output'])\n\nprint(f\"Model successfully converted to ONNX and saved at {onnx_path}\")\n","metadata":{"id":"86yLADJdO7JB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from onnx_tf.backend import prepare\n\n# Load the ONNX model\nonnx_model = onnx.load(onnx_path)\n\n# Convert ONNX to TensorFlow\ntf_rep = prepare(onnx_model)\ntf_path = \"/kaggle/working/models/tf_model\"\ntf_rep.export_graph(tf_path)\n\nprint(f\"Model successfully converted to TensorFlow and saved at {tf_path}\")\n","metadata":{"id":"bdUjFyurI4Gh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ntf_path = \"/kaggle/working/models/tf_model\"\n\n# Load the TensorFlow model\nconverter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n\n# Convert the model to TFLite\ntflite_model = converter.convert()\n\n# Save the TFLite model\ntflite_path = \"/kaggle/working/models/best_resnet18_model_light.tflite\"\nwith open(tflite_path, \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"Model successfully converted to TensorFlow Lite and saved at {tflite_path}\")","metadata":{"id":"DXL8JTm7I_rK","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:05:45.309274Z","iopub.execute_input":"2025-02-07T13:05:45.309626Z","iopub.status.idle":"2025-02-07T13:05:48.100536Z","shell.execute_reply.started":"2025-02-07T13:05:45.309597Z","shell.execute_reply":"2025-02-07T13:05:48.099620Z"}},"outputs":[{"name":"stdout","text":"Model successfully converted to TensorFlow Lite and saved at /kaggle/working/models/best_resnet18_model_light.tflite\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}